# -*- coding: utf-8 -*-
"""
å‡€å€¼æ•°æ®åº“ç®¡ç†å™¨ - TimescaleDB ç‰ˆæœ¬
====================================

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ TimescaleDB å­˜å‚¨å‡€å€¼æ•°æ®ï¼ˆè‡ªåŠ¨åˆ†åŒºè¶…è¡¨ï¼‰
2. æ”¯æŒå¢é‡æ›´æ–°ï¼ˆå‘åè¿½åŠ æ•°æ®ï¼‰
3. è‡ªåŠ¨åˆ›å»ºè¶…è¡¨å’Œå‹ç¼©ç­–ç•¥
4. é«˜æ€§èƒ½æŸ¥è¯¢å’Œå­˜å‚¨
"""

import sys
import psycopg2
from psycopg2 import sql
from psycopg2.extras import execute_values
import pandas as pd
from typing import Optional, List, Dict
import os
import warnings

# æŠ‘åˆ¶ Pandas SQLAlchemy è­¦å‘Š
warnings.filterwarnings('ignore', 
    message='pandas only supports SQLAlchemy',
    category=UserWarning)

# æ³¨æ„ï¼šç¼–ç è®¾ç½®åº”è¯¥åœ¨ç¨‹åºå…¥å£ç‚¹ï¼ˆå¦‚ app.py æˆ– run_*.pyï¼‰è¿›è¡Œ
# ä¸è¦åœ¨è¢«å¯¼å…¥çš„æ¨¡å—ä¸­é‡å¤è®¾ç½®ï¼Œå¦åˆ™ä¼šå¯¼è‡´ "I/O operation on closed file" é”™è¯¯


class NetValueTimescaleManager:
    """å‡€å€¼æ•°æ®åº“ç®¡ç†å™¨ - TimescaleDB ç‰ˆæœ¬"""
    
    # æ—¶é—´åŒºé—´ä¸è¡¨åçš„æ˜ å°„
    INTERVAL_TABLE_MAP = {
        '1m': 'net_value_1m',
        '3m': 'net_value_3m',
        '5m': 'net_value_5m',
        '15m': 'net_value_15m',
        '30m': 'net_value_30m',
        '1h': 'net_value_1h',
        '2h': 'net_value_2h',
        '4h': 'net_value_4h',
        '8h': 'net_value_8h',
        '12h': 'net_value_12h',
        '1d': 'net_value_1d',
    }
    
    # æ›´æ–°è®°å½•è¡¨å
    UPDATE_RECORD_TABLE = 'net_value_update_records'
    
    def __init__(self, host: str = 'localhost', port: int = 5432, 
                 database: str = 'trading', user: str = 'postgres', 
                 password: str = 'password'):
        """
        åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        
        å‚æ•°:
            host: æ•°æ®åº“ä¸»æœºåœ°å€
            port: æ•°æ®åº“ç«¯å£
            database: æ•°æ®åº“åç§°
            user: ç”¨æˆ·å
            password: å¯†ç 
        """
        self.host = host
        self.port = port
        self.database = database
        self.user = user
        self.password = password
        
        # æµ‹è¯•è¿æ¥
        self._test_connection()
    
    def _test_connection(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
        try:
            conn = self._get_connection()
            # æµ‹è¯•è¿æ¥çš„åŒæ—¶ï¼Œç¡®ä¿æ›´æ–°è®°å½•è¡¨å­˜åœ¨
            self._create_update_record_table_if_not_exists(conn)
            conn.close()
            print(f"âœ… æˆåŠŸè¿æ¥åˆ° TimescaleDB: {self.host}:{self.port}/{self.database}", flush=True)
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ° TimescaleDB: {e}", flush=True)
            print(f"   è¯·ç¡®ä¿ TimescaleDB æ­£åœ¨è¿è¡Œå¹¶ä¸”è¿æ¥ä¿¡æ¯æ­£ç¡®", flush=True)
            raise
    
    def _get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return psycopg2.connect(
            host=self.host,
            port=self.port,
            database=self.database,
            user=self.user,
            password=self.password
        )
    
    def _create_update_record_table_if_not_exists(self, conn):
        """
        åˆ›å»ºæ›´æ–°è®°å½•è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        
        è¡¨ç»“æ„ï¼š
        - address: è´¦æˆ·åœ°å€ï¼ˆä¸»é”®ï¼‰
        - first_trade_timestamp: ç¬¬ä¸€ç¬”äº¤æ˜“æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        - time_1m: 1åˆ†é’Ÿæ•°æ®æœ€åæ›´æ–°æ—¶é—´
        - time_3m: 3åˆ†é’Ÿæ•°æ®æœ€åæ›´æ–°æ—¶é—´
        - ... (æ‰€æœ‰æ—¶é—´å‘¨æœŸ)
        - time_1d: 1å¤©æ•°æ®æœ€åæ›´æ–°æ—¶é—´
        
        å‚æ•°:
            conn: æ•°æ®åº“è¿æ¥
        """
        cursor = conn.cursor()
        
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                )
            """, (self.UPDATE_RECORD_TABLE,))
            
            table_exists = cursor.fetchone()[0]
            
            if not table_exists:
                print(f"   åˆ›å»ºæ›´æ–°è®°å½•è¡¨: {self.UPDATE_RECORD_TABLE}", flush=True)
                
                # åŠ¨æ€æ„å»ºåˆ—å®šä¹‰
                columns = ["address TEXT PRIMARY KEY"]
                columns.append("first_trade_timestamp BIGINT")  # ç¬¬ä¸€ç¬”äº¤æ˜“æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
                for interval in self.INTERVAL_TABLE_MAP.keys():
                    # å°† 1m, 3m ç­‰è½¬æ¢ä¸º time_1m, time_3m
                    column_name = f"time_{interval}"
                    columns.append(f"{column_name} TIMESTAMPTZ")
                
                columns_sql = ",\n                        ".join(columns)
                
                # åˆ›å»ºè¡¨
                create_table_sql = f"""
                    CREATE TABLE {self.UPDATE_RECORD_TABLE} (
                        {columns_sql}
                    )
                """
                cursor.execute(create_table_sql)
                
                # åˆ›å»ºç´¢å¼•
                cursor.execute(f"""
                    CREATE INDEX {self.UPDATE_RECORD_TABLE}_address_idx 
                    ON {self.UPDATE_RECORD_TABLE} (address)
                """)
                
                conn.commit()
                print(f"âœ… æ›´æ–°è®°å½•è¡¨ {self.UPDATE_RECORD_TABLE} åˆ›å»ºæˆåŠŸ", flush=True)
            else:
                # è¡¨å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ first_trade_timestamp åˆ—
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.columns 
                        WHERE table_name = %s AND column_name = 'first_trade_timestamp'
                    )
                """, (self.UPDATE_RECORD_TABLE,))
                
                column_exists = cursor.fetchone()[0]
                
                if not column_exists:
                    print(f"   æ·»åŠ  first_trade_timestamp åˆ—åˆ° {self.UPDATE_RECORD_TABLE}", flush=True)
                    cursor.execute(f"""
                        ALTER TABLE {self.UPDATE_RECORD_TABLE}
                        ADD COLUMN first_trade_timestamp BIGINT
                    """)
                    conn.commit()
                    print(f"âœ… åˆ— first_trade_timestamp æ·»åŠ æˆåŠŸ", flush=True)
            
        except Exception as e:
            conn.rollback()
            print(f"âš ï¸  åˆ›å»ºæ›´æ–°è®°å½•è¡¨å¤±è´¥: {e}", flush=True)
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ç¨‹åºç»§ç»­è¿è¡Œ
    
    def _get_table_name(self, interval: str) -> str:
        """
        è·å–è¡¨å
        
        å‚æ•°:
            interval: æ—¶é—´åŒºé—´ï¼ˆå¦‚ '1h', '1d'ï¼‰
        
        è¿”å›:
            è¡¨å
        """
        if interval not in self.INTERVAL_TABLE_MAP:
            raise ValueError(f"ä¸æ”¯æŒçš„æ—¶é—´åŒºé—´: {interval}ï¼Œæ”¯æŒçš„åŒºé—´: {list(self.INTERVAL_TABLE_MAP.keys())}")
        
        return self.INTERVAL_TABLE_MAP[interval]
    
    def _create_hypertable_if_not_exists(self, conn, table_name: str, interval: str):
        """
        åˆ›å»ºè¶…è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        
        å‚æ•°:
            conn: æ•°æ®åº“è¿æ¥
            table_name: è¡¨å
            interval: æ—¶é—´åŒºé—´ï¼ˆç”¨äºè®¾ç½®åˆ†å—å¤§å°ï¼‰
        """
        cursor = conn.cursor()
        
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                )
            """, (table_name,))
            
            table_exists = cursor.fetchone()[0]
            
            if not table_exists:
                print(f"   åˆ›å»ºè¡¨: {table_name}", flush=True)
                
                # åˆ›å»ºæ™®é€šè¡¨
                cursor.execute(sql.SQL("""
                    CREATE TABLE {} (
                        address TEXT NOT NULL,
                        time TIMESTAMPTZ NOT NULL,
                        spot_account_value DOUBLE PRECISION,
                        realized_pnl DOUBLE PRECISION,
                        virtual_pnl DOUBLE PRECISION,
                        perp_account_value DOUBLE PRECISION,
                        total_assets DOUBLE PRECISION,
                        total_shares DOUBLE PRECISION,
                        net_value DOUBLE PRECISION,
                        cumulative_pnl DOUBLE PRECISION
                    )
                """).format(sql.Identifier(table_name)))
                
                # ç¡®å®šåˆ†å—å¤§å°
                chunk_time_interval = self._get_chunk_interval(interval)
                
                # è½¬æ¢ä¸ºè¶…è¡¨
                print(f"   è½¬æ¢ä¸ºè¶…è¡¨ï¼ˆåˆ†å—é—´éš”: {chunk_time_interval}ï¼‰...", flush=True)
                cursor.execute(sql.SQL("""
                    SELECT create_hypertable(
                        %s, 
                        'time',
                        chunk_time_interval => INTERVAL %s
                    )
                """), (table_name, chunk_time_interval))
                
                # åˆ›å»ºå”¯ä¸€ç´¢å¼•ï¼ˆç”¨äºé˜²æ­¢é‡å¤æ•°æ®ï¼‰
                cursor.execute(sql.SQL("""
                    CREATE UNIQUE INDEX {} 
                    ON {} (address, time DESC)
                """).format(
                    sql.Identifier(f"{table_name}_address_time_idx"),
                    sql.Identifier(table_name)
                ))
                
                print(f"   åˆ›å»ºç´¢å¼•...", flush=True)
                
                # åˆ›å»ºåœ°å€ç´¢å¼•
                cursor.execute(sql.SQL("""
                    CREATE INDEX {} 
                    ON {} (address)
                """).format(
                    sql.Identifier(f"{table_name}_address_idx"),
                    sql.Identifier(table_name)
                ))
                
                # æ·»åŠ å‹ç¼©ç­–ç•¥ï¼ˆæ—§æ•°æ®è‡ªåŠ¨å‹ç¼©ï¼‰
                print(f"   æ·»åŠ å‹ç¼©ç­–ç•¥ï¼ˆ7å¤©åè‡ªåŠ¨å‹ç¼©ï¼‰...", flush=True)
                cursor.execute(sql.SQL("""
                    ALTER TABLE {} SET (
                        timescaledb.compress,
                        timescaledb.compress_segmentby = 'address'
                    )
                """).format(sql.Identifier(table_name)))
                
                cursor.execute(sql.SQL("""
                    SELECT add_compression_policy(%s, INTERVAL '7 days')
                """), (table_name,))
                
                # æ·»åŠ æ•°æ®ä¿ç•™ç­–ç•¥ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸åˆ é™¤ï¼‰
                # cursor.execute(sql.SQL("""
                #     SELECT add_retention_policy(%s, INTERVAL '2 years')
                # """), (table_name,))
                
                conn.commit()
                print(f"âœ… è¶…è¡¨ {table_name} åˆ›å»ºæˆåŠŸ", flush=True)
            
        except Exception as e:
            conn.rollback()
            print(f"âš ï¸  åˆ›å»ºè¶…è¡¨å¤±è´¥: {e}", flush=True)
            raise
    
    def _get_chunk_interval(self, interval: str) -> str:
        """
        æ ¹æ®æ—¶é—´åŒºé—´ç¡®å®šåˆ†å—å¤§å°
        
        å‚æ•°:
            interval: æ—¶é—´åŒºé—´
        
        è¿”å›:
            PostgreSQL interval å­—ç¬¦ä¸²
        """
        # åˆ†å—ç­–ç•¥ï¼šå­˜å‚¨çº¦ 1000-2000 æ¡è®°å½•çš„æ—¶é—´è·¨åº¦
        chunk_map = {
            '1m': '1 day',      # 1440 æ¡/å¤©
            '3m': '3 days',     # 1440 æ¡/3å¤©
            '5m': '5 days',     # 1440 æ¡/5å¤©
            '15m': '7 days',    # 672 æ¡/å‘¨
            '30m': '14 days',   # 672 æ¡/ä¸¤å‘¨
            '1h': '1 month',    # 720 æ¡/æœˆ
            '2h': '2 months',   # 720 æ¡/ä¸¤æœˆ
            '4h': '3 months',   # 540 æ¡/å­£åº¦
            '8h': '6 months',   # 540 æ¡/åŠå¹´
            '12h': '1 year',    # 730 æ¡/å¹´
            '1d': '1 year',     # 365 æ¡/å¹´
        }
        return chunk_map.get(interval, '7 days')
    
    def get_latest_timestamp(self, address: str, interval: str) -> Optional[int]:
        """
        è·å–æŒ‡å®šåœ°å€çš„æœ€æ–°æ—¶é—´æˆ³
        
        å‚æ•°:
            address: è´¦æˆ·åœ°å€
            interval: æ—¶é—´åŒºé—´
        
        è¿”å›:
            æœ€æ–°æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼Œå¦‚æœæ²¡æœ‰æ•°æ®åˆ™è¿”å› Noneï¼‰
        """
        table_name = self._get_table_name(interval)
        
        conn = self._get_connection()
        try:
            cursor = conn.cursor()
            
            # æŸ¥è¯¢æœ€å¤§æ—¶é—´æˆ³
            cursor.execute(sql.SQL("""
                SELECT EXTRACT(EPOCH FROM MAX(time)) * 1000
                FROM {}
                WHERE address = %s
            """).format(sql.Identifier(table_name)), (address,))
            
            result = cursor.fetchone()
            return int(result[0]) if result[0] is not None else None
            
        finally:
            conn.close()
    
    def save_net_value_data(
        self, 
        address: str, 
        interval: str, 
        df: pd.DataFrame,
        incremental: bool = True
    ) -> Dict[str, int]:
        """
        ä¿å­˜å‡€å€¼æ•°æ®åˆ°æ•°æ®åº“
        
        å‚æ•°:
            address: è´¦æˆ·åœ°å€
            interval: æ—¶é—´åŒºé—´
            df: åŒ…å«å‡€å€¼æ•°æ®çš„ DataFrameï¼ˆå¿…é¡»åŒ…å«æŒ‡å®šçš„åˆ—ï¼‰
            incremental: æ˜¯å¦å¢é‡æ›´æ–°ï¼ˆTrue: åªè¿½åŠ æ–°æ•°æ®ï¼ŒFalse: è¦†ç›–æ‰€æœ‰æ•°æ®ï¼‰
        
        è¿”å›:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ {'inserted': æ’å…¥æ•°é‡, 'skipped': è·³è¿‡æ•°é‡, 'total': æ€»æ•°é‡}
        """
        # éªŒè¯å¿…éœ€çš„åˆ—
        required_columns = [
            'timestamp', 'spot_account_value', 'realized_pnl', 'virtual_pnl',
            'perp_account_value', 'total_assets', 'total_shares', 'net_value',
            'cumulative_pnl'
        ]
        
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"DataFrame ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
        
        # è·å–è¡¨å
        table_name = self._get_table_name(interval)
        
        # è¿æ¥æ•°æ®åº“
        conn = self._get_connection()
        
        try:
            # åˆ›å»ºè¶…è¡¨
            self._create_hypertable_if_not_exists(conn, table_name, interval)
            
            cursor = conn.cursor()
            
            # å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œè·å–æœ€æ–°æ—¶é—´æˆ³ï¼ˆç›´æ¥åœ¨å½“å‰è¿æ¥ä¸­æŸ¥è¯¢ï¼Œé¿å…åˆ›å»ºæ–°è¿æ¥ï¼‰
            latest_timestamp = None
            if incremental:
                cursor.execute(sql.SQL("""
                    SELECT EXTRACT(EPOCH FROM MAX(time)) * 1000
                    FROM {}
                    WHERE address = %s
                """).format(sql.Identifier(table_name)), (address,))
                
                result = cursor.fetchone()
                latest_timestamp = int(result[0]) if result[0] is not None else None
                
                if latest_timestamp is not None:
                    print(f"   æ•°æ®åº“ä¸­å·²æœ‰æ•°æ®ï¼Œæœ€æ–°æ—¶é—´æˆ³: {latest_timestamp}", flush=True)
            
            # å‡†å¤‡è¦æ’å…¥çš„æ•°æ®
            df_to_insert = df.copy()
            
            # ç¡®ä¿ timestamp æ˜¯æ•´æ•°ç±»å‹
            df_to_insert['timestamp'] = df_to_insert['timestamp'].astype('int64')
            
            # å¦‚æœæ˜¯å¢é‡æ›´æ–°ä¸”æœ‰å†å²æ•°æ®ï¼Œåªä¿ç•™æ–°æ•°æ®
            skipped_count = 0
            if incremental and latest_timestamp is not None:
                original_count = len(df_to_insert)
                df_to_insert = df_to_insert[df_to_insert['timestamp'] > latest_timestamp]
                skipped_count = original_count - len(df_to_insert)
                
                if skipped_count > 0:
                    print(f"   è·³è¿‡å·²å­˜åœ¨çš„ {skipped_count} æ¡æ•°æ®", flush=True)
            
            # å¦‚æœæ²¡æœ‰æ–°æ•°æ®ï¼Œæ›´æ–°è®°å½•è¡¨åè¿”å›
            if len(df_to_insert) == 0:
                print(f"   â„¹ï¸  æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ’å…¥", flush=True)
                
                # å³ä½¿æ²¡æœ‰æ–°æ•°æ®ï¼Œä¹Ÿæ›´æ–°è®°å½•è¡¨ï¼ˆè®°å½•æ£€æŸ¥æ—¶é—´ï¼‰
                import time
                current_timestamp = int(time.time() * 1000)
                try:
                    self.update_record_time(address, interval, current_timestamp)
                    print(f"   âœ… æ›´æ–°è®°å½•æ—¶é—´: {interval} -> {current_timestamp} (å½“å‰æ—¶é—´ï¼Œæ— æ–°æ•°æ®)", flush=True)
                except Exception as e:
                    print(f"   âš ï¸  æ›´æ–°è®°å½•æ—¶é—´å¤±è´¥: {e}", flush=True)
                
                return {'inserted': 0, 'skipped': skipped_count, 'total': len(df)}
            
            # å¦‚æœä¸æ˜¯å¢é‡æ›´æ–°ï¼Œå…ˆåˆ é™¤è¯¥åœ°å€çš„æ‰€æœ‰æ•°æ®
            if not incremental:
                cursor.execute(sql.SQL("DELETE FROM {} WHERE address = %s").format(
                    sql.Identifier(table_name)
                ), (address,))
                deleted_count = cursor.rowcount
                if deleted_count > 0:
                    print(f"   å·²åˆ é™¤ {deleted_count} æ¡æ—§æ•°æ®", flush=True)
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
            print(f"   å¼€å§‹æ’å…¥ {len(df_to_insert)} æ¡æ•°æ®...", flush=True)
            
            # ä½¿ç”¨å‘é‡åŒ–æ“ä½œå‡†å¤‡æ•°æ®ï¼ˆæ¯” iterrows å¿«10-100å€ï¼‰
            # å°† NaN æ›¿æ¢ä¸º None
            df_prepared = df_to_insert[['timestamp', 'spot_account_value', 'realized_pnl', 
                                        'virtual_pnl', 'perp_account_value', 'total_assets',
                                        'total_shares', 'net_value', 'cumulative_pnl']].copy()
            df_prepared = df_prepared.where(pd.notna(df_prepared), None)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆå‘é‡åŒ–æ“ä½œï¼Œéå¸¸å¿«ï¼‰
            values_list = df_prepared.values.tolist()
            
            # ä¸ºæ¯è¡Œæ·»åŠ  addressï¼ˆä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ï¼Œæ¯”å¾ªç¯å¿«ï¼‰
            data_list = [(address, int(row[0]), *row[1:]) for row in values_list]
            
            # ä½¿ç”¨ execute_values æ‰¹é‡æ’å…¥ï¼ˆé«˜æ€§èƒ½ï¼‰
            insert_query_str = sql.SQL("""
                INSERT INTO {} (address, time, spot_account_value, realized_pnl, 
                               virtual_pnl, perp_account_value, total_assets, 
                               total_shares, net_value, cumulative_pnl)
                VALUES %s
                ON CONFLICT (address, time) DO NOTHING
            """).format(sql.Identifier(table_name)).as_string(conn)
            
            # æ‰¹é‡æ’å…¥ï¼ˆæ¯æ‰¹1000æ¡ï¼Œæ¯5æ‰¹commitä¸€æ¬¡ä»¥æé«˜æ€§èƒ½ï¼‰
            batch_size = 1000
            commit_interval = 5  # æ¯5æ‰¹commitä¸€æ¬¡
            inserted_count = 0
            batches_since_commit = 0
            
            for i in range(0, len(data_list), batch_size):
                batch_data = data_list[i:i+batch_size]
                
                # ä½¿ç”¨ execute_values æ‰¹é‡æ’å…¥
                execute_values(
                    cursor,
                    insert_query_str,
                    batch_data,
                    template="(%s, to_timestamp(%s::double precision / 1000), %s, %s, %s, %s, %s, %s, %s, %s)"
                )
                
                batches_since_commit += 1
                inserted_count += len(batch_data)
                
                # æ¯5æ‰¹æˆ–æœ€åä¸€æ‰¹æ—¶æäº¤äº‹åŠ¡
                if batches_since_commit >= commit_interval or (i + batch_size) >= len(data_list):
                    conn.commit()
                    batches_since_commit = 0
                
                # æ˜¾ç¤ºè¿›åº¦
                progress_pct = (min(i + batch_size, len(data_list)) / len(data_list)) * 100
                print(f"   å·²æ’å…¥ {min(i + batch_size, len(data_list))}/{len(data_list)} æ¡æ•°æ® ({progress_pct:.1f}%)...", flush=True)
            
            # æ’å…¥æ•°æ®åï¼Œæ›´æ–°è®°å½•è¡¨
            # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºæ›´æ–°æ—¶é—´ï¼ˆè®°å½•ä»€ä¹ˆæ—¶å€™æ‰§è¡Œçš„æ›´æ–°æ“ä½œï¼‰
            import time
            current_timestamp = int(time.time() * 1000)  # å½“å‰æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            latest_data_timestamp = int(df_to_insert['timestamp'].max())  # æ•°æ®çš„æœ€æ–°æ—¶é—´
            
            try:
                self.update_record_time(address, interval, current_timestamp)
                print(f"   âœ… æ›´æ–°è®°å½•æ—¶é—´: {interval} -> {current_timestamp} (å½“å‰æ—¶é—´)", flush=True)
                print(f"   ğŸ“Š æ•°æ®æœ€æ–°æ—¶é—´: {latest_data_timestamp}", flush=True)
            except Exception as e:
                print(f"   âš ï¸  æ›´æ–°è®°å½•æ—¶é—´å¤±è´¥: {e}", flush=True)
            
            return {
                'inserted': inserted_count,
                'skipped': skipped_count,
                'total': len(df)
            }
            
        except Exception as e:
            conn.rollback()
            raise Exception(f"ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        
        finally:
            conn.close()
    
    def query_net_value_data(
        self,
        address: str,
        interval: str,
        start_timestamp: Optional[int] = None,
        end_timestamp: Optional[int] = None
    ) -> pd.DataFrame:
        """
        æŸ¥è¯¢å‡€å€¼æ•°æ®
        
        å‚æ•°:
            address: è´¦æˆ·åœ°å€
            interval: æ—¶é—´åŒºé—´
            start_timestamp: èµ·å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼Œå¯é€‰ï¼‰
            end_timestamp: ç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼Œå¯é€‰ï¼‰
        
        è¿”å›:
            åŒ…å«å‡€å€¼æ•°æ®çš„ DataFrame
        """
        table_name = self._get_table_name(interval)
        
        conn = self._get_connection()
        
        try:
            # æ„å»ºæŸ¥è¯¢è¯­å¥ï¼ˆä½¿ç”¨å­—ç¬¦ä¸²æ‹¼æ¥æ„å»ºåŠ¨æ€WHEREæ¡ä»¶ï¼‰
            where_conditions = ["address = %s"]
            params = [address]
            
            if start_timestamp is not None:
                where_conditions.append("time >= to_timestamp(%s::double precision / 1000)")
                params.append(start_timestamp)
            
            if end_timestamp is not None:
                where_conditions.append("time <= to_timestamp(%s::double precision / 1000)")
                params.append(end_timestamp)
            
            # åªç­›é€‰æœ‰æ•ˆæ•°æ®ï¼ˆtotal_shares > 0ï¼‰
            where_conditions.append("total_shares > 0")
            
            where_clause = " AND ".join(where_conditions)
            
            # æ„å»ºå®Œæ•´SQLï¼ˆä½¿ç”¨sql.SQLç¡®ä¿å®‰å…¨ï¼‰
            # ä½¿ç”¨çª—å£å‡½æ•°åœ¨SQLå±‚é¢è¿‡æ»¤ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ªéé›¶ cumulative_pnlï¼Œä»è¯¥ç‚¹å¼€å§‹è¿”å›æ•°æ®
            query = sql.SQL("""
                WITH ranked_data AS (
                    SELECT 
                        address,
                        EXTRACT(EPOCH FROM time) * 1000 as timestamp,
                        spot_account_value,
                        realized_pnl,
                        virtual_pnl,
                        perp_account_value,
                        total_assets,
                        total_shares,
                        net_value,
                        cumulative_pnl,
                        ROW_NUMBER() OVER (ORDER BY time ASC) as rn
                    FROM {}
                    WHERE {}
                ),
                first_nonzero AS (
                    SELECT MIN(rn) as min_rn
                    FROM ranked_data
                    WHERE ABS(cumulative_pnl) > 0.000001
                )
                SELECT 
                    address,
                    timestamp,
                    spot_account_value,
                    realized_pnl,
                    virtual_pnl,
                    perp_account_value,
                    total_assets,
                    total_shares,
                    net_value,
                    cumulative_pnl
                FROM ranked_data
                WHERE rn >= COALESCE((SELECT min_rn FROM first_nonzero), 1)
                ORDER BY timestamp ASC
            """).format(
                sql.Identifier(table_name),
                sql.SQL(where_clause)
            )
            query_str = query.as_string(conn)
            
            # æŸ¥è¯¢æ•°æ®ï¼ˆpd.read_sql_queryä¼šè‡ªåŠ¨å¤„ç†å‚æ•°åŒ–æŸ¥è¯¢ï¼‰
            df = pd.read_sql_query(query_str, conn, params=params)
            
            return df
            
        finally:
            conn.close()
    
    def get_table_stats(self, interval: str) -> Dict:
        """
        è·å–è¡¨çš„ç»Ÿè®¡ä¿¡æ¯
        
        å‚æ•°:
            interval: æ—¶é—´åŒºé—´
        
        è¿”å›:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        table_name = self._get_table_name(interval)
        
        conn = self._get_connection()
        
        try:
            cursor = conn.cursor()
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                )
            """, (table_name,))
            
            if not cursor.fetchone()[0]:
                return {
                    'exists': False,
                    'total_records': 0,
                    'address_count': 0,
                    'earliest_timestamp': None,
                    'latest_timestamp': None,
                    'chunks': 0,
                    'compressed_chunks': 0,
                    'total_size': '0 B',
                    'compressed_size': '0 B'
                }
            
            # è·å–æ€»è®°å½•æ•°
            cursor.execute(sql.SQL("SELECT COUNT(*) FROM {}").format(
                sql.Identifier(table_name)
            ))
            total_records = cursor.fetchone()[0]
            
            # è·å–åœ°å€æ•°é‡
            cursor.execute(sql.SQL("SELECT COUNT(DISTINCT address) FROM {}").format(
                sql.Identifier(table_name)
            ))
            address_count = cursor.fetchone()[0]
            
            # è·å–æ—¶é—´èŒƒå›´
            cursor.execute(sql.SQL("""
                SELECT 
                    EXTRACT(EPOCH FROM MIN(time)) * 1000,
                    EXTRACT(EPOCH FROM MAX(time)) * 1000
                FROM {}
            """).format(sql.Identifier(table_name)))
            result = cursor.fetchone()
            earliest_timestamp = int(result[0]) if result[0] is not None else None
            latest_timestamp = int(result[1]) if result[1] is not None else None
            
            # è·å–åˆ†å—ä¿¡æ¯
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_chunks,
                    COUNT(*) FILTER (WHERE is_compressed) as compressed_chunks
                FROM timescaledb_information.chunks
                WHERE hypertable_name = %s
            """, (table_name,))
            chunks_result = cursor.fetchone()
            total_chunks = chunks_result[0] if chunks_result[0] else 0
            compressed_chunks = chunks_result[1] if chunks_result[1] else 0
            
            # è·å–è¡¨å¤§å°
            cursor.execute(sql.SQL("""
                SELECT 
                    pg_size_pretty(pg_total_relation_size(%s)) as total_size,
                    pg_size_pretty(
                        pg_total_relation_size(%s) - 
                        COALESCE(
                            (SELECT SUM(pg_total_relation_size(format('%%I.%%I', chunk_schema, chunk_name)))
                             FROM timescaledb_information.chunks
                             WHERE hypertable_name = %s AND NOT is_compressed),
                            0
                        )
                    ) as compressed_size
            """), (table_name, table_name, table_name))
            size_result = cursor.fetchone()
            
            return {
                'exists': True,
                'total_records': total_records,
                'address_count': address_count,
                'earliest_timestamp': earliest_timestamp,
                'latest_timestamp': latest_timestamp,
                'chunks': total_chunks,
                'compressed_chunks': compressed_chunks,
                'total_size': size_result[0] if size_result else '0 B',
                'compressed_size': size_result[1] if size_result else '0 B'
            }
            
        finally:
            conn.close()
    
    def list_addresses(self, interval: str) -> List[str]:
        """
        åˆ—å‡ºæŒ‡å®šæ—¶é—´åŒºé—´è¡¨ä¸­çš„æ‰€æœ‰åœ°å€
        
        å‚æ•°:
            interval: æ—¶é—´åŒºé—´
        
        è¿”å›:
            åœ°å€åˆ—è¡¨
        """
        table_name = self._get_table_name(interval)
        
        conn = self._get_connection()
        
        try:
            cursor = conn.cursor()
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                )
            """, (table_name,))
            
            if not cursor.fetchone()[0]:
                return []
            
            # æŸ¥è¯¢æ‰€æœ‰åœ°å€
            cursor.execute(sql.SQL("SELECT DISTINCT address FROM {}").format(
                sql.Identifier(table_name)
            ))
            addresses = [row[0] for row in cursor.fetchall()]
            
            return addresses
            
        finally:
            conn.close()
    
    def list_all_addresses(self) -> Dict[str, List[str]]:
        """
        ä¸€æ¬¡æ€§åˆ—å‡ºæ‰€æœ‰æ—¶é—´åŒºé—´è¡¨ä¸­çš„åœ°å€ï¼ˆæ‰¹é‡æŸ¥è¯¢ï¼Œæ€§èƒ½ä¼˜åŒ–ï¼‰
        
        ä¼˜åŒ–ï¼šç›´æ¥ä»æ›´æ–°è®°å½•è¡¨æŸ¥è¯¢ï¼Œè€Œä¸æ˜¯éå†11ä¸ªè¡¨
        
        è¿”å›:
            å­—å…¸ï¼Œé”®ä¸ºæ—¶é—´åŒºé—´ï¼Œå€¼ä¸ºåœ°å€åˆ—è¡¨
            ä¾‹å¦‚: {'1h': ['addr1', 'addr2'], '1d': ['addr3']}
        """
        conn = self._get_connection()
        result = {}
        
        try:
            cursor = conn.cursor()
            
            # æ£€æŸ¥æ›´æ–°è®°å½•è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                )
            """, (self.UPDATE_RECORD_TABLE,))
            
            if cursor.fetchone()[0]:
                # ä¼˜åŒ–ï¼šç›´æ¥ä»æ›´æ–°è®°å½•è¡¨æŸ¥è¯¢ï¼ˆä¸€æ¬¡æŸ¥è¯¢è·å–æ‰€æœ‰ä¿¡æ¯ï¼‰
                # æ„å»ºæŸ¥è¯¢åˆ—
                columns = ['address']
                for interval in self.INTERVAL_TABLE_MAP.keys():
                    columns.append(f"time_{interval}")
                
                columns_sql = ", ".join(columns)
                
                query_sql = f"""
                    SELECT {columns_sql}
                    FROM {self.UPDATE_RECORD_TABLE}
                """
                
                cursor.execute(query_sql)
                rows = cursor.fetchall()
                
                # åˆå§‹åŒ–ç»“æœå­—å…¸
                for interval in self.INTERVAL_TABLE_MAP.keys():
                    result[interval] = []
                
                # éå†æ¯ä¸ªåœ°å€ï¼Œæ ¹æ®æ—¶é—´åˆ—æ˜¯å¦ä¸ºNULLåˆ¤æ–­è¯¥åœ°å€åœ¨å“ªäº›å‘¨æœŸæœ‰æ•°æ®
                for row in rows:
                    address = row[0]
                    for idx, interval in enumerate(self.INTERVAL_TABLE_MAP.keys(), start=1):
                        # row[idx] æ˜¯å¯¹åº”çš„ time_xx åˆ—
                        if row[idx] is not None:  # å¦‚æœè¯¥å‘¨æœŸæœ‰æ›´æ–°è®°å½•
                            result[interval].append(address)
                
                return result
            else:
                # é™çº§ï¼šå¦‚æœæ›´æ–°è®°å½•è¡¨ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹æ³•
                print("âš ï¸  æ›´æ–°è®°å½•è¡¨ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æŸ¥è¯¢åœ°å€", flush=True)
                
                for interval, table_name in self.INTERVAL_TABLE_MAP.items():
                    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                    cursor.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_name = %s
                        )
                    """, (table_name,))
                    
                    if cursor.fetchone()[0]:
                        # æŸ¥è¯¢æ‰€æœ‰åœ°å€
                        cursor.execute(sql.SQL("SELECT DISTINCT address FROM {}").format(
                            sql.Identifier(table_name)
                        ))
                        addresses = [row[0] for row in cursor.fetchall()]
                        result[interval] = addresses
                    else:
                        result[interval] = []
                
                return result
            
        finally:
            conn.close()
    
    def update_record_time(self, address: str, interval: str, update_time: Optional[int] = None):
        """
        æ›´æ–°æŒ‡å®šåœ°å€å’Œæ—¶é—´å‘¨æœŸçš„æ›´æ–°è®°å½•æ—¶é—´
        
        å‚æ•°:
            address: è´¦æˆ·åœ°å€
            interval: æ—¶é—´åŒºé—´
            update_time: æ›´æ–°æ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¶é—´ï¼‰
        """
        if interval not in self.INTERVAL_TABLE_MAP:
            raise ValueError(f"ä¸æ”¯æŒçš„æ—¶é—´åŒºé—´: {interval}")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ›´æ–°æ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
        if update_time is None:
            import time
            update_time = int(time.time() * 1000)
        
        conn = self._get_connection()
        
        try:
            cursor = conn.cursor()
            
            # åˆ—åï¼štime_1m, time_3m ç­‰
            column_name = f"time_{interval}"
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºè¦æ›´æ–°çš„å†…å®¹
            from datetime import datetime
            update_time_str = datetime.fromtimestamp(update_time / 1000).strftime('%Y-%m-%d %H:%M:%S')
            print(f"   ğŸ”„ å‡†å¤‡æ›´æ–°è®°å½•: {column_name} = {update_time_str} (åœ°å€: {address[:10]}...)", flush=True)
            
            # ä½¿ç”¨ UPSERTï¼ˆINSERT ... ON CONFLICTï¼‰è¯­æ³•
            # å¦‚æœåœ°å€ä¸å­˜åœ¨ï¼Œæ’å…¥æ–°è®°å½•ï¼›å¦‚æœå­˜åœ¨ï¼Œæ›´æ–°å¯¹åº”çš„æ—¶é—´åˆ—
            upsert_sql = f"""
                INSERT INTO {self.UPDATE_RECORD_TABLE} (address, {column_name})
                VALUES (%s, to_timestamp(%s::double precision / 1000))
                ON CONFLICT (address) 
                DO UPDATE SET {column_name} = EXCLUDED.{column_name}
            """
            
            cursor.execute(upsert_sql, (address, update_time))
            rows_affected = cursor.rowcount
            conn.commit()
            
            # éªŒè¯æ›´æ–°æ˜¯å¦æˆåŠŸ
            cursor.execute(f"""
                SELECT EXTRACT(EPOCH FROM {column_name}) * 1000 
                FROM {self.UPDATE_RECORD_TABLE}
                WHERE address = %s
            """, (address,))
            result = cursor.fetchone()
            
            if result and result[0]:
                actual_timestamp = int(result[0])
                actual_time_str = datetime.fromtimestamp(actual_timestamp / 1000).strftime('%Y-%m-%d %H:%M:%S')
                
                if abs(actual_timestamp - update_time) < 1000:  # å…è®¸1ç§’è¯¯å·®
                    print(f"   âœ… æ›´æ–°æˆåŠŸéªŒè¯: {column_name} = {actual_time_str}", flush=True)
                else:
                    print(f"   âš ï¸  æ›´æ–°åéªŒè¯å¤±è´¥: æœŸæœ› {update_time_str}ï¼Œå®é™… {actual_time_str}", flush=True)
            else:
                print(f"   âš ï¸  æ›´æ–°åæŸ¥è¯¢ä¸ºç©º", flush=True)
            
        except Exception as e:
            conn.rollback()
            print(f"   âŒ æ›´æ–°è®°å½•æ—¶é—´å¤±è´¥: {e}", flush=True)
            import traceback
            traceback.print_exc()
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
        
        finally:
            conn.close()
    
    def get_update_record(self, address: str) -> Optional[Dict[str, Optional[int]]]:
        """
        è·å–æŒ‡å®šåœ°å€çš„æ‰€æœ‰æ›´æ–°è®°å½•
        
        å‚æ•°:
            address: è´¦æˆ·åœ°å€
        
        è¿”å›:
            å­—å…¸ï¼Œé”®ä¸ºæ—¶é—´åŒºé—´ï¼Œå€¼ä¸ºæœ€åæ›´æ–°æ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰ï¼Œå¦‚æœæœªæ›´æ–°è¿‡åˆ™ä¸ºNone
            ä¾‹å¦‚: {'1m': 1704067200000, '3m': None, '5m': 1704070800000, ...}
            å¦‚æœåœ°å€ä¸å­˜åœ¨ï¼Œè¿”å›None
        """
        conn = self._get_connection()
        
        try:
            cursor = conn.cursor()
            
            # æ„å»ºæŸ¥è¯¢åˆ—
            columns = ["address"]
            for interval in self.INTERVAL_TABLE_MAP.keys():
                column_name = f"time_{interval}"
                # è½¬æ¢ä¸ºæ¯«ç§’æ—¶é—´æˆ³
                columns.append(f"EXTRACT(EPOCH FROM {column_name}) * 1000 as {column_name}")
            
            columns_sql = ", ".join(columns)
            
            query_sql = f"""
                SELECT {columns_sql}
                FROM {self.UPDATE_RECORD_TABLE}
                WHERE address = %s
            """
            
            cursor.execute(query_sql, (address,))
            result = cursor.fetchone()
            
            if result is None:
                return None
            
            # æ„å»ºè¿”å›å­—å…¸
            record = {}
            for idx, interval in enumerate(self.INTERVAL_TABLE_MAP.keys(), start=1):
                # result[0] æ˜¯ addressï¼Œä» result[1] å¼€å§‹æ˜¯æ—¶é—´åˆ—
                timestamp_value = result[idx]
                record[interval] = int(timestamp_value) if timestamp_value is not None else None
            
            return record
            
        finally:
            conn.close()
    
    def get_all_update_records(self) -> Dict[str, Dict[str, Optional[int]]]:
        """
        è·å–æ‰€æœ‰åœ°å€çš„æ›´æ–°è®°å½•
        
        è¿”å›:
            åµŒå¥—å­—å…¸ï¼Œå¤–å±‚é”®ä¸ºåœ°å€ï¼Œå†…å±‚é”®ä¸ºæ—¶é—´åŒºé—´ï¼Œå€¼ä¸ºæœ€åæ›´æ–°æ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰
            ä¾‹å¦‚: {
                '0x123...': {'1m': 1704067200000, '3m': None, ...},
                '0x456...': {'1m': 1704070800000, '3m': 1704074400000, ...}
            }
        """
        conn = self._get_connection()
        
        try:
            cursor = conn.cursor()
            
            # æ„å»ºæŸ¥è¯¢åˆ—
            columns = ["address"]
            for interval in self.INTERVAL_TABLE_MAP.keys():
                column_name = f"time_{interval}"
                columns.append(f"EXTRACT(EPOCH FROM {column_name}) * 1000 as {column_name}")
            
            columns_sql = ", ".join(columns)
            
            query_sql = f"""
                SELECT {columns_sql}
                FROM {self.UPDATE_RECORD_TABLE}
                ORDER BY address
            """
            
            cursor.execute(query_sql)
            results = cursor.fetchall()
            
            # æ„å»ºè¿”å›å­—å…¸
            all_records = {}
            for row in results:
                address = row[0]
                record = {}
                for idx, interval in enumerate(self.INTERVAL_TABLE_MAP.keys(), start=1):
                    timestamp_value = row[idx]
                    record[interval] = int(timestamp_value) if timestamp_value is not None else None
                all_records[address] = record
            
            return all_records
            
        finally:
            conn.close()
    
    def update_first_trade_timestamp(self, address: str, first_trade_timestamp: int):
        """
        æ›´æ–°æŒ‡å®šåœ°å€çš„ç¬¬ä¸€ç¬”äº¤æ˜“æ—¶é—´æˆ³
        
        å‚æ•°:
            address: è´¦æˆ·åœ°å€
            first_trade_timestamp: ç¬¬ä¸€ç¬”äº¤æ˜“æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        """
        conn = self._get_connection()
        
        try:
            cursor = conn.cursor()
            
            # ä½¿ç”¨ UPSERT
            upsert_sql = f"""
                INSERT INTO {self.UPDATE_RECORD_TABLE} (address, first_trade_timestamp)
                VALUES (%s, %s)
                ON CONFLICT (address) 
                DO UPDATE SET first_trade_timestamp = EXCLUDED.first_trade_timestamp
                WHERE {self.UPDATE_RECORD_TABLE}.first_trade_timestamp IS NULL
                   OR {self.UPDATE_RECORD_TABLE}.first_trade_timestamp > EXCLUDED.first_trade_timestamp
            """
            
            cursor.execute(upsert_sql, (address, first_trade_timestamp))
            conn.commit()
            
            from datetime import datetime
            time_str = datetime.fromtimestamp(first_trade_timestamp / 1000).strftime('%Y-%m-%d %H:%M:%S')
            print(f"   âœ… æ›´æ–°ç¬¬ä¸€ç¬”äº¤æ˜“æ—¶é—´: {time_str}", flush=True)
            
        except Exception as e:
            conn.rollback()
            print(f"   âš ï¸  æ›´æ–°ç¬¬ä¸€ç¬”äº¤æ˜“æ—¶é—´å¤±è´¥: {e}", flush=True)
        
        finally:
            conn.close()
    
    def get_first_trade_timestamp(self, address: str) -> Optional[int]:
        """
        è·å–æŒ‡å®šåœ°å€çš„ç¬¬ä¸€ç¬”äº¤æ˜“æ—¶é—´æˆ³
        
        å‚æ•°:
            address: è´¦æˆ·åœ°å€
        
        è¿”å›:
            ç¬¬ä¸€ç¬”äº¤æ˜“æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        conn = self._get_connection()
        
        try:
            cursor = conn.cursor()
            
            cursor.execute(f"""
                SELECT first_trade_timestamp
                FROM {self.UPDATE_RECORD_TABLE}
                WHERE address = %s
            """, (address,))
            
            result = cursor.fetchone()
            
            if result and result[0]:
                return int(result[0])
            return None
            
        finally:
            conn.close()
    
    def check_data_exists(self, address: str, interval: str) -> Dict:
        """
        å¿«é€Ÿæ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨ï¼ˆä»æ›´æ–°è®°å½•è¡¨æŸ¥è¯¢ï¼‰
        
        å‚æ•°:
            address: è´¦æˆ·åœ°å€
            interval: æ—¶é—´åŒºé—´
        
        è¿”å›:
            {
                'exists': True/False,
                'last_update': æœ€åæ›´æ–°æ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸ºNoneï¼‰
            }
        """
        if interval not in self.INTERVAL_TABLE_MAP:
            raise ValueError(f"ä¸æ”¯æŒçš„æ—¶é—´åŒºé—´: {interval}")
        
        conn = self._get_connection()
        
        try:
            cursor = conn.cursor()
            
            # æ£€æŸ¥æ›´æ–°è®°å½•è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                )
            """, (self.UPDATE_RECORD_TABLE,))
            
            if not cursor.fetchone()[0]:
                # æ›´æ–°è®°å½•è¡¨ä¸å­˜åœ¨ï¼Œé™çº§åˆ°æŸ¥è¯¢æ•°æ®è¡¨
                table_name = self._get_table_name(interval)
                cursor.execute(sql.SQL("""
                    SELECT COUNT(*)
                    FROM {}
                    WHERE address = %s AND total_shares > 0
                """).format(sql.Identifier(table_name)), (address,))
                
                count = cursor.fetchone()[0]
                return {
                    'exists': count > 0,
                    'last_update': None
                }
            
            # ä»æ›´æ–°è®°å½•è¡¨æŸ¥è¯¢
            column_name = f"time_{interval}"
            cursor.execute(f"""
                SELECT EXTRACT(EPOCH FROM {column_name}) * 1000
                FROM {self.UPDATE_RECORD_TABLE}
                WHERE address = %s
            """, (address,))
            
            result = cursor.fetchone()
            
            if result and result[0]:
                # æœ‰æ›´æ–°è®°å½•ï¼Œè¯´æ˜æœ‰æ•°æ®
                return {
                    'exists': True,
                    'last_update': int(result[0])
                }
            else:
                # æ²¡æœ‰æ›´æ–°è®°å½•ï¼Œè¯´æ˜æ²¡æœ‰æ•°æ®
                return {
                    'exists': False,
                    'last_update': None
                }
            
        finally:
            conn.close()

